{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OoJackoO/MMAI894_Deep-Learning-Course-Project/blob/dev/Com2_Load_Tomato.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyNdop7XUGDt"
      },
      "source": [
        " \n",
        "\n",
        "Data Source: \n",
        "\n",
        "https://www.kaggle.com/datasets/cookiefinder/tomato-disease-multiple-sources  \n",
        "\n",
        "Training set: train folder \n",
        "\n",
        "Validation and Test set: valid folder \n",
        "\n",
        "Reference: https://cs230.stanford.edu/blog/split/  \n",
        "\n",
        "Random seed: 24 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnIXbZGbY15_"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "# Add modules as needed\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import layers, regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, Dropout, BatchNormalization, LayerNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvQW68qYOQXP",
        "outputId": "2270fb9e-bd82-4a75-c41a-da1ec3e0b6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/MMAI_894/Team_Assignment/'\n",
        "#!kaggle datasets download -d cookiefinder/tomato-disease-multiple-sources --path \"/content/drive/MyDrive/MMAI_894/Team_Assignment/Dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is just to get a list of dirs for the visualization and EDA functions\n",
        "def define_paths(dir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    #images = []\n",
        "    folds = os.listdir(dir)\n",
        "    print(\"The unique images labels are:\", folds)\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        print(foldpath)\n",
        "        print(len(filelist),\"files\")\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(fold)\n",
        "    filepaths_array = np.array(filepaths)\n",
        "    labels_array = np.array(labels)\n",
        "\n",
        "    return filepaths_array, labels_array\n",
        "\n",
        "data_dir = ('/content/drive/MyDrive/MMAI_894/Team_Assignment/Dataset/tomato-disease-multiple-sources_Data/train')\n",
        "filepaths_array, labels_array = define_paths(data_dir)\n",
        "print(filepaths_array.shape)\n",
        "print(labels_array.shape)\n",
        "print(filepaths_array[-1])\n",
        "print(labels_array[-1])\n"
      ],
      "metadata": {
        "id": "NyT1XccVYWs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "YdVZYMBgi02K",
        "outputId": "00adf3af-afce-4145-8b93-088748c2ecf8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3aa6eba20855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepaths_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filepaths_array' is not defined"
          ]
        }
      ],
      "source": [
        "def visualize(filepaths_array,labels_array):\n",
        "  num_images = len(filepaths_array)\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=num_images, figsize=(20,20))\n",
        "  print(num_images)\n",
        "# Display each image on a separate axis\n",
        "  for i, images_array in enumerate(filepaths_array):\n",
        "      #print(filepaths_array[i])\n",
        "      image = mpimg.imread(filepaths_array[i])\n",
        "      height, width, channels = image.shape\n",
        "      ax[i].imshow(image)\n",
        "      print(height, width, channels)\n",
        "      #ax[i].set_xticks([])\n",
        "      #ax[i].set_yticks([])\n",
        "      ax[i].set_title(labels_array[i])\n",
        "\n",
        "stop = len(filepaths_array)\n",
        "start = stop-6\n",
        "\n",
        "visualize(filepaths_array[start:stop], labels_array[start:stop])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "modEX11A7zrs",
        "outputId": "7151bae2-24bd-43bd-b239-b9c9443b0ff6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-91595350ffdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdata_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels_array' is not defined"
          ]
        }
      ],
      "source": [
        "def data_distribution(labels_array):\n",
        "  # Get the count of each label\n",
        "  unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "\n",
        "# Use a bar chart to show the distribution of labels\n",
        "  bars = plt.bar(np.arange(len(counts)), counts)\n",
        "  plt.xticks(np.arange(len(counts)), unique_labels, rotation=90)\n",
        "  plt.tick_params(labeltop=True, labelbottom=False)\n",
        "  plt.xlabel('Class')\n",
        "  plt.ylabel('Count')\n",
        "  plt.bar_label(bars, labels=counts, label_type='edge', fontsize=10)\n",
        "  plt.show()\n",
        "\n",
        "data_distribution(labels_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# This functions builds a generator -> splits images into batches, normalizes each image, reshapes each image to the specified size, extracts the category classes, one hot encoded the catergories\n",
        "def loadandsplit_images(dir):\n",
        "\n",
        "  data_dir = dir\n",
        "  data_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
        "  \n",
        "  random_seed = 24\n",
        "  batch_size = 128\n",
        "\n",
        "  train_generator = data_datagen.flow_from_directory(\n",
        "      data_dir,\n",
        "      target_size = (256,256),\n",
        "      batch_size = batch_size,\n",
        "      class_mode = 'categorical',\n",
        "      subset='training',\n",
        "      seed=random_seed\n",
        "  )\n",
        "  \n",
        "  val_generator = data_datagen.flow_from_directory(\n",
        "      data_dir,\n",
        "      target_size = (256,256),\n",
        "      batch_size = batch_size,\n",
        "      class_mode = 'categorical',\n",
        "      subset='validation',\n",
        "      seed=random_seed\n",
        "  )\n",
        "\n",
        "  total_samples = train_generator.samples + val_generator.samples\n",
        "  print(\"\\nTotal number of samples:\", total_samples)\n",
        "  print(\"Number of training image samples:\", train_generator.samples)\n",
        "  print(\"Number of validation image samples:\", val_generator.samples)\n",
        "  print(\"\\nX_train % split:\", train_generator.samples / total_samples*100,\"%\")\n",
        "  print(\"X_val % split:\", val_generator.samples / total_samples*100,\"%\")\n",
        "  print(\"Batch size:\", batch_size)\n",
        "\n",
        "\n",
        "  return train_generator, val_generator\n",
        "\n",
        "data_dir = ('/content/drive/MyDrive/MMAI_894/Team_Assignment/Dataset/tomato-disease-multiple-sources_Data/train')\n",
        "train_gen, val_gen = loadandsplit_images(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljkgP32RUc9c",
        "outputId": "0e6ba9fa-9996-4550-d92f-a84f2f6d3992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12714 images belonging to 11 classes.\n",
            "Found 12709 images belonging to 11 classes.\n",
            "\n",
            "Total number of samples: 25423\n",
            "Number of training image samples: 12714\n",
            "Number of validation image samples: 12709\n",
            "\n",
            "X_train % split: 50.009833615230306 %\n",
            "X_val % split: 49.990166384769694 %\n",
            "Batch size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMjlr8ySDsT0",
        "outputId": "cbb971ba-9eba-4650-a41f-177f2b076f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Convolution_Layer1 (Conv2D)  (None, 256, 256, 32)     1568      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 128, 128, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " Convolution_Layer2 (Conv2D)  (None, 128, 128, 64)     32832     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 64, 64, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Convolution_Layer3 (Conv2D)  (None, 64, 64, 64)       65600     \n",
            "                                                                 \n",
            " Flatten_layer (Flatten)     (None, 262144)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 262144)            0         \n",
            "                                                                 \n",
            " Output (Dense)              (None, 11)                2883595   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,983,595\n",
            "Trainable params: 2,983,595\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(None, 262144)\n"
          ]
        }
      ],
      "source": [
        "def build_model():\n",
        "    # TODO: build the model, \n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "    \n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=32, kernel_size = (4, 4), activation = 'relu', input_shape=(256, 256, 3),strides = (1,1), padding = \"same\", name=\"Convolution_Layer1\"))\n",
        "    model.add(layers.MaxPool2D(2,2))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(4, 4), activation = 'relu', strides=(1,1), padding=\"same\", name=\"Convolution_Layer2\"))\n",
        "    model.add(layers.MaxPool2D(2,2))\n",
        "    model.add(Conv2D(64, (4, 4), activation = 'relu', strides=(1,1), padding=\"same\",name=\"Convolution_Layer3\"))\n",
        "    model.add(layers.Flatten(name=\"Flatten_layer\"))\n",
        "\n",
        "    \n",
        "    #Layer 1 with 64 nodes and relu activation. Input will get calcualted by Keras\n",
        "    # A single Dense layer looked to give suffecient performance as compared to adding two layers i.e. not too much benefit was observed in this case by having two dense layers\n",
        "    #model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001), name=\"Layer_1\"))\n",
        "    #Add dropout of 0.30 (this number was arrived upon via hyperparameter tuning)\n",
        "    model.add(Dropout(0.30))\n",
        "    #Added an Batch normalization layer to help improve performance - this didn't help\n",
        "    #model.add(BatchNormalization())\n",
        "    #Output with softmax activation and 11 out out layers corresponding to the output classes\n",
        "    model.add(Dense(11, activation='softmax',  name=\"Output\"))\n",
        "    #Lets see how the model looks\n",
        "    model.summary()   \n",
        "    print(model.get_layer(\"Flatten_layer\").output_shape)\n",
        "\n",
        "    return model\n",
        "\n",
        "def compile_model(model):\n",
        "    # TODO: compile the model\n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "\n",
        "    #Lets try different optimizers as well as learning rates to see if it helps\n",
        "    optimizer = Adam()\n",
        "    #optimizer = Adam(learning_rate=0.001)\n",
        "    #optimizer = RMSprop()\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model = compile_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpkKMWDIDy9M",
        "outputId": "e71c5268-aff5-42e2-e541-30800c1061be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.6991 - accuracy: 0.4262 "
          ]
        }
      ],
      "source": [
        "def train_model(model, XY_train, XY_val, batch_size, epochs):\n",
        "    # TODO: train the model\n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "\n",
        "    # Lets add early stopping if the val loss doesn't improve after 3 epochs\n",
        "    # early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "    checkpoint = ModelCheckpoint('best_weights.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "\n",
        "    # Using 12-20 epochs and including validation data. Also added verbose to monitor output with early stopping. \n",
        "    # Can use early stopping as well buthave it commented out for this instance\n",
        "    # history = model.fit(X_train, Y_train, epochs=20, batch_size=64,verbose=1, validation_data=(X_val, Y_val), callbacks=[early_stopping, checkpoint])\n",
        "    history = model.fit(XY_train, \n",
        "                        epochs=epochs, batch_size=batch_size, verbose=1, \n",
        "                        validation_data=(XY_val), validation_steps=XY_val.samples // batch_size,\n",
        "                        callbacks=[checkpoint])\n",
        "\n",
        "    return model, history\n",
        "\n",
        "model, history = train_model(model, train_gen, val_gen, batch_size=128, epochs=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E3QLFnwEHbZ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, X_test, Y_test):\n",
        "    # TODO: evaluate the model\n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "    model.load_weights('best_weights.h5')\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "## You may use this space (and add additional cells for exploration)\n",
        "\n",
        "model = build_model()\n",
        "model = compile_model(model)\n",
        "model, history = train_model(model, X_train, Y_train, X_val, Y_val)\n",
        "test_loss, test_accuracy = eval_model(model, X_test, Y_test)\n",
        "\n",
        "# added this line as want to see what the predicted classification for an input of X_test is\n",
        "output = model.predict(X_test)\n",
        "\n",
        "print(\"\\nX_test Accuracy = \", test_accuracy*100,\"%\")\n",
        "print(\"X_test Loss = \", test_loss*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Everything after this is either depracated or experimental"
      ],
      "metadata": {
        "id": "nbBqY7h3YXAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_24qQHtErkc",
        "outputId": "f080ddef-9954-423b-e17f-2e68955af466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(12714, 11)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "#you don't need this as the generator automatically one hot encoded the labels\n",
        "\n",
        "def one_hot_data(train_data, val_data):\n",
        "\n",
        "  _ , batch_labels = next(train_data)\n",
        "  print(batch_labels)\n",
        "  train_labels = train_data.classes\n",
        "  classes = train_data.num_classes\n",
        "  one_hot_train_labels = to_categorical(train_labels, num_classes=classes)\n",
        "   \n",
        "  val_labels = train_data.classes\n",
        "  one_hot_val_labels = to_categorical(val_labels, num_classes=classes)\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "  return one_hot_train_labels, one_hot_val_labels\n",
        "\n",
        "\n",
        "Y_train, Y_val = one_hot_data(train_gen, val_gen)\n",
        "print(Y_train.shape)\n",
        "print(Y_train[12713])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "167DE1PMpN2XABWMY0prrZbCxvJyQSD31",
      "authorship_tag": "ABX9TyOUWlYzOF4sV2vSWB3NiKoC",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}