{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "g5aOVPfnEP3p",
        "NQU4SYs31RVl",
        "7CRMxYe_U0XF",
        "m6f66HnVXjoi",
        "CWXAusW7Iias",
        "w6VhfmOiekTZ",
        "z2bSXIJWgRsf",
        "IEgtxMjqhH9p",
        "uSLuHTRTiC0i"
      ],
      "mount_file_id": "14wjkwrpLbg2ttoFqXGJJAU3YGNmX5ttN",
      "authorship_tag": "ABX9TyOv8jr1gQrd7yQRoIQXrNQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OoJackoO/MMAI894_Deep-Learning-Course-Project/blob/dev/MMAI894_Tomato_Disease_CLS_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading & Loading the Dataset"
      ],
      "metadata": {
        "id": "IUAQ4aubCBAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huUQr-u5TjX7"
      },
      "outputs": [],
      "source": [
        "# !pip install -q kaggle\n",
        "# ! mkdir ~/content/drive/My Drive/Colab Notebooks/MMAI 894\n",
        "# ! cp kaggle.json ~/content/drive/My Drive/Colab Notebooks/MMAI 894/\n",
        "# ! chmod 600 ~/kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "jYk09u_GOBvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Set the directory to where you want to have your dataset & jason authentication file\n",
        "# import os\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/Colab Notebooks/MMAI 894/Dataset\"\n",
        "# %cd /content/drive/My Drive/Colab Notebooks/MMAI 894/Dataset/"
      ],
      "metadata": {
        "id": "vab1_pq12dls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "# import zipfile"
      ],
      "metadata": {
        "id": "wRkviHwaz4X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # this will prompt you to upload the kaggle.json\n",
        "# from google.colab import files\n",
        "\n",
        "# files.upload() "
      ],
      "metadata": {
        "id": "E5xsEk5c6n6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check if your jason file is there and authentication is successful\n",
        "# api = KaggleApi()\n",
        "# api.authenticate()\n",
        "# !ls -lha kaggle.json"
      ],
      "metadata": {
        "id": "r1qTUG5p6a-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ! kaggle competitions download google-smartphone-decimeter-challenge\n",
        "# !kaggle datasets download -d cookiefinder/tomato-disease-multiple-sources\n",
        "# with zipfile.ZipFile('tomato-disease-multiple-sources.zip', 'r') as zip_ref:\n",
        "#         zip_ref.extractall('./')"
      ],
      "metadata": {
        "id": "X1oTWsRp2qHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Modules and Importing Dataset"
      ],
      "metadata": {
        "id": "g5aOVPfnEP3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import cv2\n",
        "# import time\n",
        "# import shutil\n",
        "# import itertools\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import tensorflow as tf\n",
        "# sns.set_style('darkgrid')\n",
        "# from tensorflow import keras\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras import regularizers\n",
        "# from tensorflow.keras.optimizers import Adam, Adamax\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from tensorflow.keras.metrics import categorical_crossentropy\n",
        "# from tensorflow.keras.models import Model, load_model, Sequential\n",
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "# print ('modules loaded')"
      ],
      "metadata": {
        "id": "G7QZxVGBPKZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import shutil\n",
        "import itertools\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "from PIL import ImageFile\n",
        "\n",
        "import PIL\n",
        "import PIL.Image\n",
        "\n",
        "from pathlib import Path\n",
        "import imghdr\n",
        "\n",
        "import tqdm, re, json\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet import ResNet101\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.densenet import DenseNet201\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "print ('modules loaded')"
      ],
      "metadata": {
        "id": "1z7McUjE9XhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6200b649-e03a-4100-ac49-ea273a99296d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modules loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the directory to your specific folder in Google Drive\n",
        "folder_path = \"/content/drive/My Drive/Colab Notebooks/MMAI 894/Dataset/\"\n",
        "\n",
        "# Check if the directory exists and print the list of files and folders inside it\n",
        "if os.path.exists(folder_path):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(files)\n",
        "else:\n",
        "    print(\"Directory does not exist.\")\n",
        "\n",
        "root_dir = \"/content/drive/My Drive/\" \n",
        "# choose where you want your project files to be saved\n",
        "project_folder = \"Colab Notebooks/MMAI 894/Dataset/\"\n",
        "os.chdir(root_dir + project_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqxH1ym3fuiw",
        "outputId": "176b8d7a-9abf-4b3d-fb1c-ab107aac4023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['tomato-disease-multiple-sources.zip', 'train', 'valid']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlxUiPzph8xS",
        "outputId": "edc2fbe9-74e1-4086-d6a2-918cc3af7995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MMAI 894/Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Datasets from Drive Datasets"
      ],
      "metadata": {
        "id": "NQU4SYs31RVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = os.getcwd()\n",
        "train_dir = os.path.join(main_dir, 'train')\n",
        "val_dir = os.path.join(main_dir, 'valid')"
      ],
      "metadata": {
        "id": "7I6a7weA2Cl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/load_data/images"
      ],
      "metadata": {
        "id": "EDHEciYs1nSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=os.listdir(train_dir),\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    seed=24,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "iSZXo5RDrrPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48738a8-6061-404a-c7a0-2e8add32d21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25821 files belonging to 11 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Original_val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    seed=24,\n",
        "    validation_split=0.5,\n",
        "    subset='both',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "3hqH4ssHtkq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebda6f22-1d93-4a20-a375-c47c7b17016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6677 files belonging to 11 classes.\n",
            "Using 3339 files for training.\n",
            "Using 3338 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = Original_val_ds[0]\n",
        "test_ds = Original_val_ds[1]"
      ],
      "metadata": {
        "id": "EHaZGVqI04l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_save = train_ds\n",
        "val_ds_save = val_ds\n",
        "test_ds_save = test_ds"
      ],
      "metadata": {
        "id": "73sS1LRCToRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "yrBtgkStsQfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89517ab7-cf40-4770-d151-421397dbf233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 224, 224, 3)\n",
            "(32, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "-cvrsaN81aOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6596c3e-2ba1-4505-e4b5-2e3b29ba17a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bacterial_spot', 'Early_blight', 'Late_blight', 'Leaf_Mold', 'Septoria_leaf_spot', 'Spider_mites Two-spotted_spider_mite', 'Target_Spot', 'Tomato_Yellow_Leaf_Curl_Virus', 'Tomato_mosaic_virus', 'healthy', 'powdery_mildew']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delete Bad Quality Images"
      ],
      "metadata": {
        "id": "7CRMxYe_U0XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_bad_quality = False # Only need to runn the first time"
      ],
      "metadata": {
        "id": "okYAoGXVzvQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_len, test_len = 0, 0\n",
        "for f in os.listdir(train_dir):\n",
        "  train_len += len(os.listdir(os.path.join(train_dir, f)))\n",
        "  test_len += len(os.listdir(os.path.join(val_dir, f)))\n",
        "print(train_len);print(test_len)"
      ],
      "metadata": {
        "id": "TqcMDr8-7ah9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0acc69-0404-4f29-dca8-b5b9770a113e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25821\n",
            "6677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_quality_check(image_dir):\n",
        "  '''\n",
        "  This functions checks whether file formats accepted by Tensorflow ( JPEG, PNG, GIF, BMP), or may be corrupted.\n",
        "  '''\n",
        "  from pathlib import Path\n",
        "  import imghdr\n",
        "\n",
        "  image_extensions = ['.jpg', '.png', '.jpeg', '.PNG', '.JPG', '.JPEG']\n",
        "  img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
        "  bad_images = []\n",
        "\n",
        "  for filepath in Path(image_dir).rglob(\"*\"):\n",
        "    if filepath.suffix.lower() in image_extensions:\n",
        "        img_type = imghdr.what(filepath)\n",
        "        if img_type is None:\n",
        "            print(f\"{filepath} is not an image\")\n",
        "            bad_images.append(filepath)\n",
        "        elif img_type not in img_type_accepted_by_tf:\n",
        "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n",
        "            bad_images.append(filepath)\n",
        "  return bad_images"
      ],
      "metadata": {
        "id": "vZukpq2eU_LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_quality_check2(image_dir):\n",
        "  '''\n",
        "  This function captures corrupted image files (jpg and png) that may missed by first QA function.\n",
        "  '''\n",
        "  import os, glob\n",
        "\n",
        "  # validate directory\n",
        "  assert os.path.exists(image_dir), \"Error: directory does not exist\"\n",
        "\n",
        "  bad_images = []\n",
        "  classes = sorted([c for c in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, c))])\n",
        "\n",
        "  def is_valid_jpg(jpg_file):\n",
        "    with open(jpg_file, 'rb') as f:\n",
        "      f.seek(-2,2)\n",
        "      buf = f.read()\n",
        "      return buf == b'\\xff\\xd9'\n",
        "\n",
        "  def is_valid_png(png_file):\n",
        "    with open(png_file, 'rb') as f:\n",
        "      f.seek(-2,2)\n",
        "      but = f.read()\n",
        "      return but == b'\\x60\\x82'\n",
        "\n",
        "  for f_class in classes:\n",
        "    class_path = os.path.join(image_dir, f_class)\n",
        "    file_list = os.listdir(class_path)\n",
        "    desc = f'{f_class:23s}'\n",
        "    for file in tqdm.tqdm(file_list, ncols=110, desc=desc, unit='file', colour='blue'):\n",
        "      fpath = os.path.join(class_path, file)\n",
        "      index = file.rfind('.')\n",
        "      if file[index:] in ['.jpg', '.jpeg','.JPG', '.JPEG']:\n",
        "        if not is_valid_jpg(fpath):\n",
        "          print(fpath)\n",
        "          bad_images.append(fpath)\n",
        "      \n",
        "      if file[index:] in ['.png', '.PNG']:\n",
        "        if not is_valid_png(fpath):\n",
        "          print(fpath)\n",
        "          bad_images.append(fpath)\n",
        "\n",
        "  return bad_images"
      ],
      "metadata": {
        "id": "5NQ5Eot_VECV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_quality_check3(image_dir):\n",
        "  '''\n",
        "  This function focuses on identifying corrupted png images that may be missed by the QA1 and QA2.\n",
        "  '''\n",
        "  assert os.path.exists(image_dir), \"Error: directory does not exist\"\n",
        "\n",
        "  bad_images = []\n",
        "  classes = sorted([c for c in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, c))])\n",
        "\n",
        "  for f_class in classes:\n",
        "    class_path = os.path.join(image_dir, f_class)\n",
        "    file_list = os.listdir(class_path)\n",
        "    desc = f'{f_class:23s}'\n",
        "    for file in tqdm.tqdm(file_list, ncols=110, desc=desc, unit='file', colour='blue'):\n",
        "      fpath = os.path.join(class_path, file)\n",
        "      index = file.rfind('.')\n",
        "      if file[index:] in ['.png', '.PNG']:\n",
        "        try:\n",
        "          image = tf.io.read_file(fpath)\n",
        "          image = tf.image.decode_png(image, channels=3)\n",
        "        except:\n",
        "          bad_images.append(fpath)\n",
        "          print('defective image file: ', fpath)\n",
        "  return bad_images"
      ],
      "metadata": {
        "id": "8iVbPqnOVN7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if remove_bad_quality:\n",
        "  # check image quality\n",
        "  %time\n",
        "  print(\"-\"*20, \"check training set image quality.\",\"-\"*20)\n",
        "  bad_images_train = image_quality_check(train_dir)\n",
        "  print(\"-\"*20, \"check test and validation set image quality.\",\"-\"*20)\n",
        "  bad_images_val_test = image_quality_check(val_dir)\n"
      ],
      "metadata": {
        "id": "k3o3ilaPVQqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if remove_bad_quality:\n",
        "  # check image quality 2\n",
        "  %time\n",
        "  print(\"-\"*20, \"check training set image quality.\",\"-\"*20)\n",
        "  bad_images_train2 = image_quality_check2(train_dir)\n",
        "  print(\"-\"*20, \"check test and validation set image quality.\",\"-\"*20)\n",
        "  bad_images_val_test2 = image_quality_check2(val_dir)"
      ],
      "metadata": {
        "id": "9tRkf17NVTLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if remove_bad_quality:\n",
        "  # check image quality 3\n",
        "  %time\n",
        "  print(\"-\"*20, \"check training set image quality.\",\"-\"*20)\n",
        "  bad_images_train3 = image_quality_check3(train_dir)\n",
        "  print(\"-\"*20, \"check test and validation set image quality.\",\"-\"*20)\n",
        "  bad_images_val_test3 = image_quality_check3(val_dir)"
      ],
      "metadata": {
        "id": "C4nMBh8bVVzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if remove_bad_quality:\n",
        "  bad_images = set(bad_images_train + bad_images_val_test + bad_images_train2 + bad_images_val_test2 + bad_images_train3 + bad_images_val_test3)\n",
        "  bad_images\n",
        "  for link in bad_images:\n",
        "    if os.path.exists(link):\n",
        "      os.remove(str(link))\n",
        "  print('Move {} images with bad quality.'.format(len(bad_images)))\n",
        "\n"
      ],
      "metadata": {
        "id": "iZ-Q38RkVXiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_len, test_len = 0, 0\n",
        "for f in os.listdir(train_dir):\n",
        "  train_len += len(os.listdir(os.path.join(train_dir, f)))\n",
        "  test_len += len(os.listdir(os.path.join(val_dir, f)))\n",
        "print(train_len);print(test_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30H2oYtaQlGo",
        "outputId": "cf6c457b-32f8-4101-95ce-aaf50c7ab81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25821\n",
            "6677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Clean Datasets"
      ],
      "metadata": {
        "id": "m6f66HnVXjoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if remove_bad_quality:\n",
        "  train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=os.listdir(train_dir),\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    seed=24,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "tvR-rDXRXfjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if remove_bad_quality:\n",
        "  Original_val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    seed=24,\n",
        "    validation_split=0.5,\n",
        "    subset='both',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "umyNH_s1Xhey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = Original_val_ds[0]\n",
        "test_ds = Original_val_ds[1]"
      ],
      "metadata": {
        "id": "by6v8eTzXxxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train dataset size: ' + str(train_ds.cardinality().numpy()))\n",
        "print('valid dataset size: ' + str(val_ds.cardinality().numpy()))\n",
        "print('test dataset size: ' + str(test_ds.cardinality().numpy()))"
      ],
      "metadata": {
        "id": "SFMGkHS3z_6_",
        "outputId": "74d1da5c-b457-4eb9-df3d-99ae7e8c87bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset size: 807\n",
            "valid dataset size: 105\n",
            "test dataset size: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/68191448/unknown-image-file-format-one-of-jpeg-png-gif-bmp-required\n",
        "\n",
        "https://www.tensorflow.org/tutorials/load_data/images\n",
        "\n",
        "https://stackoverflow.com/questions/65438156/tensorflow-keras-error-unknown-image-file-format-one-of-jpeg-png-gif-bmp-re"
      ],
      "metadata": {
        "id": "BKkwICcD37v0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX4HcGZFIrRa"
      },
      "outputs": [],
      "source": [
        "def plot_training(hist):\n",
        "  '''\n",
        "  This function plots training history. (ie training loss vs validation loss)\n",
        "  '''\n",
        "  tr_acc = hist.history['accuracy']\n",
        "  tr_loss = hist.history['loss']\n",
        "  val_acc = hist.history['val_accuracy']\n",
        "  val_loss = hist.history['val_loss']\n",
        "  index_loss = np.argmin(val_loss)\n",
        "  val_lowest = val_loss[index_loss]\n",
        "  index_acc = np.argmax(val_acc)\n",
        "  acc_highest = val_acc[index_acc]\n",
        "\n",
        "  plt.figure(figsize= (20, 8))\n",
        "  plt.style.use('fivethirtyeight')\n",
        "  Epochs = [i+1 for i in range(len(tr_acc))]\n",
        "  loss_label = f'best epoch= {str(index_loss + 1)}'\n",
        "  acc_label = f'best epoch= {str(index_acc + 1)}'\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
        "  plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
        "  plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
        "  plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
        "  plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.tight_layout\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning With MobileNetV2"
      ],
      "metadata": {
        "id": "wHoY4e2dc9_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "          'RANDOM_STATE': 24,\n",
        "          'TRAINABLE': False,\n",
        "          'DENSE_ACTIVATION':'relu',\n",
        "          'DROPOUT_RATE':0.3,\n",
        "          'LEARNING_RATE': 1e-4,\n",
        "          'LEARNING_RATE_DECAY':False,\n",
        "          'CHECKPOINT':True,\n",
        "          'EPOCHS':50,\n",
        "          'NUM_CLASSES': len(class_names),\n",
        "}"
      ],
      "metadata": {
        "id": "hanxeDqJ2mMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping \n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    patience=7, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# check point - save best model\n",
        "check_point = ModelCheckpoint(filepath='../model/mobilenet_best_1.hdf5',\n",
        "                                          monitor='val_loss',\n",
        "                                          mode='min',\n",
        "                                          save_best_only=True\n",
        "                                          )\n",
        "\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 0.01\n",
        "\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 **(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr_start, params['EPOCHS'])\n",
        "lr_scheduler = LearningRateScheduler(exponential_decay_fn)\n",
        "     \n",
        "\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "if params['CHECKPOINT']:\n",
        "  callbacks.append(check_point)\n",
        "if params['LEARNING_RATE_DECAY']:\n",
        "  callbacks.append(lr_scheduler)\n",
        "\n",
        "callbacks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2ulIHlkUpiJ",
        "outputId": "098053c7-08ec-4d4a-8fa4-728bf998092d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.callbacks.EarlyStopping at 0x7f9d94712160>,\n",
              " <tensorflow.python.keras.callbacks.ModelCheckpoint at 0x7f9d94712040>]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224,224)"
      ],
      "metadata": {
        "id": "sYRl0rlD4qrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet = MobileNetV2(input_shape=IMAGE_SIZE+(3,), weights='imagenet', include_top=False)\n",
        "for layer in mobilenet.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "XM4CMFiIbSko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961b3ab8-2c84-4cc7-8fb2-51080af07be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in mobilenet.layers:\n",
        "  if \"BatchNormalization\" in layer.__class__.__name__:\n",
        "      layer.trainable = True"
      ],
      "metadata": {
        "id": "0IgFMVT7Glq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_mobilenet = layers.Flatten()(mobilenet.output)\n",
        "\n",
        "x_mobilenet = layers.Dense(256, activation=params['DENSE_ACTIVATION'])(x_mobilenet)\n",
        "\n",
        "x_mobilenet = layers.Dropout(rate=params['DROPOUT_RATE'])(x_mobilenet)\n",
        "\n",
        "prediction = keras.layers.Dense(len(class_names), activation='softmax')(x_mobilenet)\n",
        "\n",
        "mobilenet_model = Model(inputs=mobilenet.input, outputs=prediction)"
      ],
      "metadata": {
        "id": "9erFw-6rbgGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mobilenet_model.summary()"
      ],
      "metadata": {
        "id": "Mf82xnvB5VXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=params['LEARNING_RATE']) \n",
        "# loss = keras.losses.categorical_crossentropy(from_logits=False)\n",
        "\n",
        "mobilenet_model.compile(optimizer=optimizer,\n",
        "              loss=categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"]\n",
        "              )"
      ],
      "metadata": {
        "id": "mF9Dx0AG55wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = mobilenet_model.fit(train_ds, validation_data = val_ds, batch_size=128, epochs = params['EPOCHS'], callbacks = callbacks)"
      ],
      "metadata": {
        "id": "_L7RhG01bytk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca7aa69-daf0-499c-87c3-524de1e971bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = mobilenet_model.evaluate(test_ds)\n",
        "\n",
        "print(\"test loss: {} and test accuracy: {}\".format(test_loss, test_accuracy))"
      ],
      "metadata": {
        "id": "T4476lerH0n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_model.save('/content/drive/My Drive/Colab Notebooks/MMAI 894/MobilenetV2.h5')"
      ],
      "metadata": {
        "id": "Gu2yYkJ2Obcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_model = load_model('/content/drive/My Drive/Colab Notebooks/MMAI 894/MobilenetV2.h5')"
      ],
      "metadata": {
        "id": "9Uf_DYA0PI0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(history)"
      ],
      "metadata": {
        "id": "obNn_RCQIpYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = []\n",
        "for _, label in test_ds.map(lambda x, y: (x, y)):\n",
        "    test_labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "f19zlKriR7TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "# convert labels to numerical format\n",
        "test_labels = np.concatenate([y for x, y in test_ds])\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# predict on test dataset\n",
        "y_pred_mobilenet = mobilenet_model.predict(test_ds)\n",
        "\n",
        "# convert predictions to class labels\n",
        "y_pred_mobilenet = np.argmax(y_pred_mobilenet, axis=1)\n",
        "\n",
        "# print classification report\n",
        "print(classification_report(test_labels, y_pred_mobilenet, target_names=class_names))\n",
        "\n",
        "# create confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred_mobilenet)\n",
        "\n",
        "# plot confusion matrix using heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iT_XjyYOQoLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning With vgg16"
      ],
      "metadata": {
        "id": "w6VhfmOiekTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "          'RANDOM_STATE': 24,\n",
        "          'TRAINABLE': False,\n",
        "          'DENSE_ACTIVATION':'relu',\n",
        "          'DROPOUT_RATE':0.3,\n",
        "          'LEARNING_RATE': 5e-4,\n",
        "          'LEARNING_RATE_DECAY':False,\n",
        "          'CHECKPOINT':True,\n",
        "          'EPOCHS':50,\n",
        "          'NUM_CLASSES': len(class_names),\n",
        "}"
      ],
      "metadata": {
        "id": "IXeG9RzlekTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping \n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    patience=7, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# check point - save best model\n",
        "check_point = ModelCheckpoint(filepath='../model/vgg16_best_1.hdf5',\n",
        "                                          monitor='val_loss',\n",
        "                                          mode='min',\n",
        "                                          save_best_only=True\n",
        "                                          )\n",
        "\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 0.01\n",
        "\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 **(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr_start, params['EPOCHS'])\n",
        "lr_scheduler = LearningRateScheduler(exponential_decay_fn)\n",
        "     \n",
        "\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "if params['CHECKPOINT']:\n",
        "  callbacks.append(check_point)\n",
        "if params['LEARNING_RATE_DECAY']:\n",
        "  callbacks.append(lr_scheduler)\n",
        "\n",
        "callbacks"
      ],
      "metadata": {
        "id": "JBgSAjM0ekTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224,224)"
      ],
      "metadata": {
        "id": "AB_CLDolekTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = VGG16(input_shape=IMAGE_SIZE+(3,), weights='imagenet' ,include_top=False)\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "YRXXHmO5ekTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in vgg.layers:\n",
        "  if \"BatchNormalization\" in layer.__class__.__name__:\n",
        "      layer.trainable = True"
      ],
      "metadata": {
        "id": "mscJVak1ekTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vgg = layers.Flatten()(vgg.output)\n",
        "\n",
        "x_vgg = layers.Dense(256, activation=params['DENSE_ACTIVATION'])(x_vgg)\n",
        "\n",
        "x_vgg = layers.Dropout(rate=params['DROPOUT_RATE'])(x_vgg)\n",
        "\n",
        "prediction = keras.layers.Dense(len(class_names), activation='softmax')(x_vgg)\n",
        "\n",
        "vgg_model = Model(inputs=vgg.input, outputs=prediction)"
      ],
      "metadata": {
        "id": "rfA7J77AekTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model.summary()"
      ],
      "metadata": {
        "id": "ilCjRiBsekTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=params['LEARNING_RATE']) \n",
        "# loss = keras.losses.categorical_crossentropy(from_logits=False)\n",
        "\n",
        "vgg_model.compile(optimizer=optimizer,\n",
        "              loss=categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"]\n",
        "              )"
      ],
      "metadata": {
        "id": "3ubkP61dekTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = vgg_model.fit(train_ds, validation_data = val_ds, batch_size=128, epochs = params['EPOCHS'], callbacks = callbacks)"
      ],
      "metadata": {
        "id": "Fna6TDzKekTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = vgg_model.evaluate(test_ds)\n",
        "\n",
        "print(\"test loss: {} and test accuracy: {}\".format(test_loss, test_accuracy))"
      ],
      "metadata": {
        "id": "wXGFIQRRekTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model.save('/content/drive/My Drive/Colab Notebooks/MMAI 894/vgg16.h5')"
      ],
      "metadata": {
        "id": "yqSKCk0KekTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = load_model('/content/drive/My Drive/Colab Notebooks/MMAI 894/vgg16.h5')"
      ],
      "metadata": {
        "id": "RiNJgrlYekTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(history)"
      ],
      "metadata": {
        "id": "G6evNVj8ekTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = []\n",
        "for _, label in test_ds.map(lambda x, y: (x, y)):\n",
        "    test_labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "UoR000ZaekTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "# convert labels to numerical format\n",
        "test_labels = np.concatenate([y for x, y in test_ds])\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# predict on test dataset\n",
        "y_pred_vgg = vgg_model.predict(test_ds)\n",
        "\n",
        "# convert predictions to class labels\n",
        "y_pred_vgg = np.argmax(y_pred_vgg, axis=1)\n",
        "\n",
        "# print classification report\n",
        "print(classification_report(test_labels, y_pred_vgg, target_names=class_names))\n",
        "\n",
        "# create confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred_vgg)\n",
        "\n",
        "# plot confusion matrix using heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hxYKeh5vekTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning With resnet101"
      ],
      "metadata": {
        "id": "z2bSXIJWgRsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "          'RANDOM_STATE': 24,\n",
        "          'TRAINABLE': False,\n",
        "          'DENSE_ACTIVATION':'relu',\n",
        "          'DROPOUT_RATE':0.3,\n",
        "          'LEARNING_RATE': 1e-4,\n",
        "          'LEARNING_RATE_DECAY':False,\n",
        "          'CHECKPOINT':True,\n",
        "          'EPOCHS':50,\n",
        "          'NUM_CLASSES': len(class_names),\n",
        "}"
      ],
      "metadata": {
        "id": "nnvd54DvgRsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping \n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    patience=7, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# check point - save best model\n",
        "check_point = ModelCheckpoint(filepath='../model/resnet101_best_1.hdf5',\n",
        "                                          monitor='val_loss',\n",
        "                                          mode='min',\n",
        "                                          save_best_only=True\n",
        "                                          )\n",
        "\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 0.01\n",
        "\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 **(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr_start, params['EPOCHS'])\n",
        "lr_scheduler = LearningRateScheduler(exponential_decay_fn)\n",
        "     \n",
        "\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "if params['CHECKPOINT']:\n",
        "  callbacks.append(check_point)\n",
        "if params['LEARNING_RATE_DECAY']:\n",
        "  callbacks.append(lr_scheduler)\n",
        "\n",
        "callbacks"
      ],
      "metadata": {
        "id": "NZs5SVtYgRsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224,224)"
      ],
      "metadata": {
        "id": "FpdsSgfGgRsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet101 = ResNet101(input_shape=IMAGE_SIZE+(3,), weights='imagenet', include_top=False)\n",
        "for layer in mobilenet.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "6NCYNLsKgRsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in resnet101.layers:\n",
        "  if \"BatchNormalization\" in layer.__class__.__name__:\n",
        "      layer.trainable = True"
      ],
      "metadata": {
        "id": "3dKXkil6gRsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_resnet101 = layers.Flatten()(resnet101.output)\n",
        "\n",
        "x_resnet101 = layers.Dense(256, activation=params['DENSE_ACTIVATION'])(x_resnet101)\n",
        "\n",
        "x_resnet101 = layers.Dropout(rate=params['DROPOUT_RATE'])(x_resnet101)\n",
        "\n",
        "prediction = keras.layers.Dense(len(class_names), activation='softmax')(x_resnet101)\n",
        "\n",
        "resnet101_model = Model(inputs=mobilenet.input, outputs=prediction)"
      ],
      "metadata": {
        "id": "mEuOBrRXgRsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet101_model.summary()"
      ],
      "metadata": {
        "id": "YtFLT2KBgRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=params['LEARNING_RATE']) \n",
        "\n",
        "resnet101_model.compile(optimizer=optimizer,\n",
        "              loss=categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"]\n",
        "              )"
      ],
      "metadata": {
        "id": "rtSsS7G7gRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = resnet101_model.fit(train_ds, validation_data = val_ds, batch_size=128, epochs = params['EPOCHS'], callbacks = callbacks)"
      ],
      "metadata": {
        "id": "gc4tv2qBgRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = resnet101_model.evaluate(test_ds)\n",
        "\n",
        "print(\"test loss: {} and test accuracy: {}\".format(test_loss, test_accuracy))"
      ],
      "metadata": {
        "id": "gJqH-l7mgRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet101_model.save('/content/drive/My Drive/Colab Notebooks/MMAI 894/resnet101.h5')"
      ],
      "metadata": {
        "id": "qC-MaV_QgRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet101_model = load_model('/content/drive/My Drive/Colab Notebooks/MMAI 894/resnet101.h5')"
      ],
      "metadata": {
        "id": "ckRAWnkogRsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(history)"
      ],
      "metadata": {
        "id": "_14hQp3IgRsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = []\n",
        "for _, label in test_ds.map(lambda x, y: (x, y)):\n",
        "    test_labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "qZZCAvj1gRsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "# convert labels to numerical format\n",
        "test_labels = np.concatenate([y for x, y in test_ds])\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# predict on test dataset\n",
        "y_pred_resnet101 = resnet101_model.predict(test_ds)\n",
        "\n",
        "# convert predictions to class labels\n",
        "y_pred_resnet101 = np.argmax(y_pred_resnet101, axis=1)\n",
        "\n",
        "# print classification report\n",
        "print(classification_report(test_labels, y_pred_resnet101, target_names=class_names))\n",
        "\n",
        "# create confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred_resnet101)\n",
        "\n",
        "# plot confusion matrix using heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lLkQY71MgRsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning With inception"
      ],
      "metadata": {
        "id": "IEgtxMjqhH9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "          'RANDOM_STATE': 24,\n",
        "          'TRAINABLE': False,\n",
        "          'DENSE_ACTIVATION':'relu',\n",
        "          'DROPOUT_RATE':0.3,\n",
        "          'LEARNING_RATE': 1e-4,\n",
        "          'LEARNING_RATE_DECAY':False,\n",
        "          'CHECKPOINT':True,\n",
        "          'EPOCHS':50,\n",
        "          'NUM_CLASSES': len(class_names),\n",
        "}"
      ],
      "metadata": {
        "id": "cvm8AwbghH9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping \n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    patience=7, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# check point - save best model\n",
        "check_point = ModelCheckpoint(filepath='../model/inception_best_1.hdf5',\n",
        "                                          monitor='val_loss',\n",
        "                                          mode='min',\n",
        "                                          save_best_only=True\n",
        "                                          )\n",
        "\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 0.01\n",
        "\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 **(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr_start, params['EPOCHS'])\n",
        "lr_scheduler = LearningRateScheduler(exponential_decay_fn)\n",
        "     \n",
        "\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "if params['CHECKPOINT']:\n",
        "  callbacks.append(check_point)\n",
        "if params['LEARNING_RATE_DECAY']:\n",
        "  callbacks.append(lr_scheduler)\n",
        "\n",
        "callbacks"
      ],
      "metadata": {
        "id": "qh62oBsmhH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224,224)"
      ],
      "metadata": {
        "id": "7-IBr12NhH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception = InceptionV3(input_shape=IMAGE_SIZE+(3,), weights='imagenet', include_top=False)\n",
        "for layer in mobilenet.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "UQ1xhegGhH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in resnet101.layers:\n",
        "  if \"BatchNormalization\" in layer.__class__.__name__:\n",
        "      layer.trainable = True"
      ],
      "metadata": {
        "id": "p7XBd2fohH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_inception = layers.Flatten()(inception.output)\n",
        "\n",
        "x_inception = layers.Dense(256, activation=params['DENSE_ACTIVATION'])(x_inception)\n",
        "\n",
        "x_inception = layers.Dropout(rate=params['DROPOUT_RATE'])(x_inception)\n",
        "\n",
        "prediction = keras.layers.Dense(len(class_names), activation='softmax')(x_inception)\n",
        "\n",
        "inception_model = Model(inputs=mobilenet.input, outputs=prediction)"
      ],
      "metadata": {
        "id": "c20EB5WFhH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet101_model.summary()"
      ],
      "metadata": {
        "id": "H7PXNyjmhH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=params['LEARNING_RATE']) \n",
        "\n",
        "inception_model.compile(optimizer=optimizer,\n",
        "              loss=categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"]\n",
        "              )"
      ],
      "metadata": {
        "id": "NzT7x2P8hH9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = inception_model.fit(train_ds, validation_data = val_ds, batch_size=128, epochs = params['EPOCHS'], callbacks = callbacks)"
      ],
      "metadata": {
        "id": "GU7arNhZhH9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = inception_model.evaluate(test_ds)\n",
        "\n",
        "print(\"test loss: {} and test accuracy: {}\".format(test_loss, test_accuracy))"
      ],
      "metadata": {
        "id": "XMzeK2_zhH9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception_model.save('/content/drive/My Drive/Colab Notebooks/MMAI 894/inception_model.h5')"
      ],
      "metadata": {
        "id": "-DxF8eCnhH9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception_model = load_model('/content/drive/My Drive/Colab Notebooks/MMAI 894/inception_model.h5')"
      ],
      "metadata": {
        "id": "opHFi8SLhH9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(history)"
      ],
      "metadata": {
        "id": "QhJNVUuWhH9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = []\n",
        "for _, label in test_ds.map(lambda x, y: (x, y)):\n",
        "    test_labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "lyHt4USHhH9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "# convert labels to numerical format\n",
        "test_labels = np.concatenate([y for x, y in test_ds])\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# predict on test dataset\n",
        "y_pred_resnet101 = inception_model.predict(test_ds)\n",
        "\n",
        "# convert predictions to class labels\n",
        "y_pred_resnet101 = np.argmax(y_pred_resnet101, axis=1)\n",
        "\n",
        "# print classification report\n",
        "print(classification_report(test_labels, y_pred_resnet101, target_names=class_names))\n",
        "\n",
        "# create confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred_resnet101)\n",
        "\n",
        "# plot confusion matrix using heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P2cFPJ-AhH9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning With densenet201"
      ],
      "metadata": {
        "id": "uSLuHTRTiC0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "          'RANDOM_STATE': 24,\n",
        "          'TRAINABLE': False,\n",
        "          'DENSE_ACTIVATION':'relu',\n",
        "          'DROPOUT_RATE':0.3,\n",
        "          'LEARNING_RATE': 1e-4,\n",
        "          'LEARNING_RATE_DECAY':False,\n",
        "          'CHECKPOINT':True,\n",
        "          'EPOCHS':50,\n",
        "          'NUM_CLASSES': len(class_names),\n",
        "}"
      ],
      "metadata": {
        "id": "x7kXovj3iC0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping \n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    patience=7, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# check point - save best model\n",
        "check_point = ModelCheckpoint(filepath='../model/densenet_best_1.hdf5',\n",
        "                                          monitor='val_loss',\n",
        "                                          mode='min',\n",
        "                                          save_best_only=True\n",
        "                                          )\n",
        "\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 0.01\n",
        "\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 **(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr_start, params['EPOCHS'])\n",
        "lr_scheduler = LearningRateScheduler(exponential_decay_fn)\n",
        "     \n",
        "\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "if params['CHECKPOINT']:\n",
        "  callbacks.append(check_point)\n",
        "if params['LEARNING_RATE_DECAY']:\n",
        "  callbacks.append(lr_scheduler)\n",
        "\n",
        "callbacks"
      ],
      "metadata": {
        "id": "Nb-EwU4riC0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224,224)"
      ],
      "metadata": {
        "id": "kYf1Io5PiC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet = DenseNet201(input_shape=IMAGE_SIZE+(3,), weights='imagenet', include_top=False)\n",
        "for layer in mobilenet.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "sAVXfaFpiC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in densenet.layers:\n",
        "  if \"BatchNormalization\" in layer.__class__.__name__:\n",
        "      layer.trainable = True"
      ],
      "metadata": {
        "id": "81dy1IDfiC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_densenet = layers.Flatten()(densenet.output)\n",
        "\n",
        "x_densenet = layers.Dense(256, activation=params['DENSE_ACTIVATION'])(x_densenet)\n",
        "\n",
        "x_densenet = layers.Dropout(rate=params['DROPOUT_RATE'])(x_densenet)\n",
        "\n",
        "prediction = keras.layers.Dense(len(class_names), activation='softmax')(x_densenet)\n",
        "\n",
        "densenet_model = Model(inputs=mobilenet.input, outputs=prediction)"
      ],
      "metadata": {
        "id": "VnKb0LzZiC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet101_model.summary()"
      ],
      "metadata": {
        "id": "cTwzysVliC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=params['LEARNING_RATE']) \n",
        "\n",
        "densenet_model.compile(optimizer=optimizer,\n",
        "              loss=categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"]\n",
        "              )"
      ],
      "metadata": {
        "id": "KvaGncIZiC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = densenet_model.fit(train_ds, validation_data = val_ds, batch_size=128, epochs = params['EPOCHS'], callbacks = callbacks)"
      ],
      "metadata": {
        "id": "ZuK7zzDHiC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = densenet_model.evaluate(test_ds)\n",
        "\n",
        "print(\"test loss: {} and test accuracy: {}\".format(test_loss, test_accuracy))"
      ],
      "metadata": {
        "id": "zpmuwxMjiC0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet_model.save('/content/drive/My Drive/Colab Notebooks/MMAI 894/DenseNet201_model.h5')"
      ],
      "metadata": {
        "id": "cHH3XhQtiC0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet_model = load_model('/content/drive/My Drive/Colab Notebooks/MMAI 894/DenseNet201_model.h5')"
      ],
      "metadata": {
        "id": "LIaK_DQJiC0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(history)"
      ],
      "metadata": {
        "id": "OPTcrJ5GiC0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = []\n",
        "for _, label in test_ds.map(lambda x, y: (x, y)):\n",
        "    test_labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "NGEYlkjliC0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "# convert labels to numerical format\n",
        "test_labels = np.concatenate([y for x, y in test_ds])\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# predict on test dataset\n",
        "y_pred_densenet = densenet_model.predict(test_ds)\n",
        "\n",
        "# convert predictions to class labels\n",
        "y_pred_densenet = np.argmax(y_pred_densenet, axis=1)\n",
        "\n",
        "# print classification report\n",
        "print(classification_report(test_labels, y_pred_densenet, target_names=class_names))\n",
        "\n",
        "# create confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred_densenet)\n",
        "\n",
        "# plot confusion matrix using heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g67tmkLNiC0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}